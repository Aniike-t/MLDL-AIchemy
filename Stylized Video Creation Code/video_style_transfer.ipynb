{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IF MODULES ARE NOT INSTALLED , RUN THIS-->**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **IF ALL OF THESE ARE INSTALLED RUN ABOVE FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\TE6miniprojectmodels\\thirdprogram\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\TE6miniprojectmodels\\thirdprogram\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\TE6miniprojectmodels\\thirdprogram\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\TE6miniprojectmodels\\thirdprogram\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\TE6miniprojectmodels\\thirdprogram\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CONVERSION OF VIDEO IF REQUIRED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_720p(input_file, output_file):\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    target_width = 1280\n",
    "    target_height = 720\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 30, (target_width, target_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            resized_frame = cv2.resize(frame, (target_width, target_height))\n",
    "            out.write(resized_frame)\n",
    "            cv2.imshow('Frame', resized_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "input_file = 'testvideo.mp4'\n",
    "output_file = 'output_video_720p.mp4'\n",
    "\n",
    "convert_to_720p(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONVERT VIDEO TO STYLIZED FRAMES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ColorCorrector:\n",
    "    MAX_CHANNEL_INTENSITY = 255.0\n",
    "\n",
    "    def color_correct(self, content, generated):\n",
    "        # Ensure content and generated images have three channels (BGR)\n",
    "        content = cv2.cvtColor(content, cv2.COLOR_GRAY2BGR) if len(content.shape) == 2 else content\n",
    "        generated = cv2.cvtColor(generated, cv2.COLOR_GRAY2BGR) if len(generated.shape) == 2 else generated\n",
    "        \n",
    "        # Convert images to float32\n",
    "        content = np.array((content * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        generated = np.array((generated * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        \n",
    "        # Convert images to YCrCb color space\n",
    "        content = cv2.cvtColor(content, cv2.COLOR_BGR2YCR_CB)\n",
    "        generated = cv2.cvtColor(generated, cv2.COLOR_BGR2YCR_CB)\n",
    "\n",
    "        # Trim the generated image\n",
    "        generated = self._trim_img(generated)\n",
    "\n",
    "        # Merge intensity and color spaces\n",
    "        color_corrected = np.zeros(generated.shape, dtype=np.float32)\n",
    "        color_corrected[:, :, 0] = generated[:, :, 0]\n",
    "        color_corrected[:, :, 1] = content[:, :, 1]\n",
    "        color_corrected[:, :, 2] = content[:, :, 2]\n",
    "\n",
    "        # Convert color corrected image back to BGR and normalize\n",
    "        color_corrected_bgr = cv2.cvtColor(color_corrected, cv2.COLOR_YCR_CB2BGR) / self.MAX_CHANNEL_INTENSITY\n",
    "\n",
    "        return color_corrected_bgr\n",
    "\n",
    "    def _trim_img(self, img):\n",
    "        # Trim each channel to [0, 255]\n",
    "        img[:, :, 1] = np.clip(img[:, :, 1], 0, 255)\n",
    "        img[:, :, 2] = np.clip(img[:, :, 2], 0, 255)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.976023976023978\n",
      "131\n",
      "Progress: [========================================] 100.0% Complete\n",
      "Video processing completed.\n"
     ]
    }
   ],
   "source": [
    "# INPUT VIDEO AND STYLE HERE\n",
    "\n",
    "\n",
    "video_path = \"testvideo3.mp4\"\n",
    "output_folder = \"temporary_video_frames\"\n",
    "stylized_output_folder = \"stylized_frames_op\"\n",
    "style_img_path = \"Vincent Van Gogh - The Starry Night.jpg\"\n",
    "\n",
    "\n",
    "# max_dim = 256, 512, 1024, 2048 --- change this for quality\n",
    "\n",
    "\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(fps)\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(total_frames)\n",
    "user_defined_fps = 22\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(stylized_output_folder, exist_ok=True)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, user_defined_fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "style_image = load_img(style_img_path)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % int(fps / user_defined_fps) == 0:\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{frame_count}.jpg\"), frame)\n",
    "        content_image = load_img(os.path.join(output_folder, f\"frame_{frame_count}.jpg\"))\n",
    "        \n",
    "        # Perform stylization\n",
    "        stylized_images = hub_model(tf.constant(content_image), tf.constant(style_image))\n",
    "        stylized_image = tf.squeeze(stylized_images[0], axis=0)  # Remove extra dimension\n",
    "\n",
    "        # Display content and stylized images for debugging\n",
    "        # imshow(content_image[0], title='Content Image')\n",
    "        # imshow(stylized_image, title='Stylized Image')\n",
    "        # Convert to numpy array and save\n",
    "        stylized_image_np = tf.cast(stylized_image * 255, tf.uint8).numpy()\n",
    "        encoded_image = cv2.cvtColor(stylized_image_np, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "        cv2.imwrite(os.path.join(stylized_output_folder, f\"stylized_frame_{frame_count}.jpg\"), encoded_image)\n",
    "        # temp_style_image_path = os.path.join(stylized_output_folder, f\"stylized_frame_{frame_count}.jpg\")\n",
    "\n",
    "        output_video.write(encoded_image)\n",
    "\n",
    "    # Print progress bar\n",
    "    progress = frame_count / total_frames\n",
    "    bar_length = 40\n",
    "    progress_bar = int(bar_length * progress)\n",
    "    percent = round(progress * 100, 2)\n",
    "    bar = '=' * progress_bar + '.' * (bar_length - progress_bar)\n",
    "    print(f'\\rProgress: [{bar}] {percent}% Complete', end='', flush=True)\n",
    "\n",
    "cap.release()\n",
    "output_video.release()\n",
    "\n",
    "# Print completion message\n",
    "print(\"\\nVideo processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONVERT FRAMES BACK TO VIDEO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as: output_stylized_video3.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "stylized_images_dir = \"stylized_frames_op\"\n",
    "image_files = sorted(\n",
    "    [os.path.join(stylized_images_dir, file) for file in os.listdir(stylized_images_dir)],\n",
    "    key=lambda x: os.path.getmtime(x)\n",
    ")\n",
    "\n",
    "first_image = cv2.imread(image_files[0])\n",
    "height, width, _ = first_image.shape\n",
    "\n",
    "output_video_path = \"output_stylized_video3.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, user_defined_fps, (width, height))\n",
    "\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(image_file)\n",
    "    output_video.write(image)\n",
    "\n",
    "output_video.release()\n",
    "\n",
    "print(f\"Video saved as: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Video processing completed.\n"
     ]
    }
   ],
   "source": [
    "video_path = \"videos/testvideo.mp4\"\n",
    "output_folder = \"temporary_video_frames\"\n",
    "stylized_output_folder = \"stylized_frames_op\"\n",
    "style_img_path = \"Vincent_van_Gogh/Vincent Van Gogh - The Starry Night.jpg\"\n",
    "user_defined_fps = 25\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(stylized_output_folder, exist_ok=True)\n",
    "\n",
    "# Load style image\n",
    "style_image = load_img(style_img_path)\n",
    "\n",
    "# Load TensorFlow Hub model for style transfer\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')  # Replace with your actual path\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Create output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, user_defined_fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Process video frames\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    \n",
    "    # Stylize every nth frame according to user_defined_fps\n",
    "    if frame_count % int(fps / user_defined_fps) == 0:\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{frame_count}.jpg\"), frame)\n",
    "        content_image = load_img(os.path.join(output_folder, f\"frame_{frame_count}.jpg\"))\n",
    "\n",
    "        # Perform stylization\n",
    "        stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
    "        \n",
    "        # Convert to numpy array and save\n",
    "        stylized_image = tf.image.convert_image_dtype(stylized_image, tf.uint8)\n",
    "        stylized_image_np = tf.image.encode_jpeg(tf.cast(stylized_image[0] * 255, tf.uint8))\n",
    "        \n",
    "        # Save stylized frame\n",
    "        with open(os.path.join(stylized_output_folder, f\"stylized_frame_{frame_count}.jpg\"), 'wb') as f:\n",
    "            f.write(stylized_image_np.numpy())\n",
    "        \n",
    "        # Convert encoded JPEG bytes to numpy array\n",
    "        stylized_image_array = cv2.imdecode(np.frombuffer(stylized_image_np.numpy(), np.uint8), -1)\n",
    "\n",
    "        # Write stylized frame to output video\n",
    "        output_video.write(stylized_image_array)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "output_video.release()\n",
    "\n",
    "# Print completion message\n",
    "print(\"\\nVideo processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
